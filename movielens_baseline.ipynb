{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AybhDoonoeN",
        "outputId": "85935dd5-bf33-47ed-d14e-156ec812a58e"
      },
      "outputs": [],
      "source": [
        "# !pip3 install surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwAYTOC7vJTk"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gObZfTwMnLHS"
      },
      "outputs": [],
      "source": [
        "#Local Imports\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "from numpy import *\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tqdm as tqdm\n",
        "import requests, zipfile, io\n",
        "from collections import defaultdict\n",
        "\n",
        "#Import for Recommendation Models\n",
        "from surprise import Reader, Dataset, SVD, accuracy, SVDpp, SlopeOne, BaselineOnly, CoClustering, KNNBasic, KNNWithMeans, KNNWithZScore\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Import for visualizations\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvB42Q9xwGRq"
      },
      "source": [
        "## Configuration Chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrRU_C_vwFWA"
      },
      "outputs": [],
      "source": [
        "movies_datapath = 'ml-latest-small/movies.csv'\n",
        "ratings_datapath = 'ml-latest-small/ratings.csv'\n",
        "chunk_size = 500000\n",
        "result_dir = 'results/'\n",
        "os.makedirs(result_dir,exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgEKOrz_F4VK"
      },
      "source": [
        "## Dataset helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFC2cT7CnNRP"
      },
      "outputs": [],
      "source": [
        "# Loading the mapping data which is to map each movie Id\n",
        "# in the ratings with it's title and genre\n",
        "# the resulted data structure is a dictionary where the\n",
        "# movie id is the key, the genre and titles are values\n",
        "def load_mapping_data():\n",
        "    movie_data = {}\n",
        "    df_dtype = {\n",
        "        \"movieId\": int,\n",
        "        \"title\": str,\n",
        "        \"genres\": str\n",
        "    }\n",
        "    cols = list(df_dtype.keys())\n",
        "    for df_chunk in tqdm.tqdm(pd.read_csv(movies_datapath, usecols=cols, dtype=df_dtype, chunksize=chunk_size)):\n",
        "        combine_data = [list(a) for a in\n",
        "                        zip(df_chunk[\"movieId\"].tolist(), df_chunk[\"title\"].tolist(),\n",
        "                            df_chunk[\"genres\"].tolist())]\n",
        "        for a in combine_data:\n",
        "            movie_data[a[0]] = [a[1], a[2]]\n",
        "    del df_chunk\n",
        "    return movie_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHmVE06wnQdc"
      },
      "outputs": [],
      "source": [
        "# Loading the rating data which is around 27M records it takes around 2 minutes\n",
        "# the resulted data structure us a dictionary where the\n",
        "# user id is the key and all their raings are values for example for user 1 :\n",
        "# 1 = {\n",
        "#     [movieId,rating,timestamp],\n",
        "#     [movieId,rating,timestamp],\n",
        "#     [movieId,rating,timestamp],\n",
        "#   }\n",
        "\n",
        "def load_data():\n",
        "    rating_data = {}\n",
        "    unique_user_id = []\n",
        "    chunk_size = 50000\n",
        "    df_dtype = {\n",
        "        \"userId\": int,\n",
        "        \"movieId\": int,\n",
        "        \"rating\": float,\n",
        "        \"timestamp\": int,\n",
        "    }\n",
        "    cols = list(df_dtype.keys())\n",
        "    for df_chunk in tqdm.tqdm(pd.read_csv(ratings_datapath, usecols=cols, dtype=df_dtype, chunksize=chunk_size)):\n",
        "        user_id = df_chunk[\"userId\"].tolist()\n",
        "        unique_user_id.extend(set(user_id))\n",
        "        movie_id = df_chunk[\"movieId\"].tolist()\n",
        "        rating = df_chunk[\"rating\"].tolist()\n",
        "        timestamp = df_chunk[\"timestamp\"].tolist()\n",
        "        combine_data = [list(a) for a in zip(user_id, movie_id, rating, timestamp)]\n",
        "        for a in combine_data:\n",
        "            if a[0] in rating_data.keys():\n",
        "                rating_data[a[0]].extend([[a[0], a[1], a[2], a[3]]])\n",
        "            else:\n",
        "                rating_data[a[0]] = [[a[0], a[1], a[2], a[3]]]\n",
        "    del df_chunk\n",
        "    \n",
        "    return rating_data, unique_user_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1BosR_TnVB5"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing\n",
        "# this processes isn't being done for the whole dataset instead it's being done\n",
        "# for each user id, for each user we split their ratings 80 training and 20 testing\n",
        "# the resulted training and testing datasets are including the whole original dataset\n",
        "def spilt_data(rating_data, unique_user_id):\n",
        "    training_data = []\n",
        "    testing_data = []\n",
        "    t0 = time.time()\n",
        "    t1 = time.time()\n",
        "    for u in unique_user_id:\n",
        "        if len(rating_data[u]) == 1:\n",
        "            x_test = rating_data[u]\n",
        "            x_train = rating_data[u]\n",
        "        else:\n",
        "            x_train, x_test = train_test_split(rating_data[u], test_size=0.2)\n",
        "        training_data.extend(x_train)\n",
        "        testing_data.extend(x_test)\n",
        "    total = t1 - t0\n",
        "    print(int(total))\n",
        "    return training_data, testing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnOZbxU4nXWm"
      },
      "outputs": [],
      "source": [
        "def get_movie_title(movie_id, movie_data):\n",
        "    if movie_id in movie_data.keys():\n",
        "        return movie_data[movie_id][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZSJZEfdnZo9"
      },
      "outputs": [],
      "source": [
        "def get_movie_genre(movie_id, movie_data):\n",
        "    if movie_id in movie_data.keys():\n",
        "        return movie_data[movie_id][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_8W4PGRgxQX"
      },
      "source": [
        "\n",
        "## Get train test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8stgRArnbML"
      },
      "outputs": [],
      "source": [
        "def get_train_test_data(new_sample = False):\n",
        "    if new_sample:\n",
        "        rating_data, unique_user_id = load_data()\n",
        "        training_data, testing_data = spilt_data(rating_data, unique_user_id)\n",
        "        training_dataframe = pd.DataFrame.from_records(training_data)\n",
        "        training_dataframe.columns = [\"userId\",\"movieId\",\"rating\",\"timestamp\"]\n",
        "        testing_dataframe = pd.DataFrame.from_records(testing_data)\n",
        "        testing_dataframe.columns=[\"userId\",\"movieId\",\"rating\",\"timestamp\"]\n",
        "\n",
        "        file = open('training_dataframe', 'wb')\n",
        "        pickle.dump(training_dataframe, file)\n",
        "        file.close()\n",
        "\n",
        "        file = open('testing_dataframe', 'wb')\n",
        "        pickle.dump(testing_dataframe, file)\n",
        "        file.close()\n",
        "\n",
        "    else:\n",
        "        file = open('training_dataframe', 'rb')\n",
        "        training_dataframe = pickle.load(file)\n",
        "        file.close()\n",
        "\n",
        "        file = open('testing_dataframe', 'rb')\n",
        "        testing_dataframe = pickle.load(file)\n",
        "        file.close()\n",
        "\n",
        "    return training_dataframe, testing_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nNXqvIvqFNC"
      },
      "outputs": [],
      "source": [
        "def convert_traintest_dataframe_forsurprise(training_dataframe, testing_dataframe):\n",
        "\ttraining_dataframe = training_dataframe.iloc[:, :-1]\n",
        "\ttesting_dataframe = testing_dataframe.iloc[:, :-1]\n",
        "\treader = Reader(rating_scale=(0,5))\n",
        "\ttrainset = Dataset.load_from_df(training_dataframe[['userId', 'movieId', 'rating']], reader)\n",
        "\ttestset = Dataset.load_from_df(testing_dataframe[['userId', 'movieId', 'rating']], reader)\n",
        "\t# trainset = trainset.construct_trainset(trainset.raw_ratings)\n",
        "\ttestset = testset.construct_testset(testset.raw_ratings)\n",
        "\treturn trainset, testset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DoPOH-ZGHte"
      },
      "source": [
        "## do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjvOcSRCUm8b"
      },
      "outputs": [],
      "source": [
        "svd_param_grid = {'n_factors':[50,100,150],\n",
        "                  'n_epochs': [20, 25], \n",
        "                  'lr_all': [0.007, 0.009, 0.01],\n",
        "                  'reg_all': [0.4, 0.6],\n",
        "                  'sim_options': {'name': ['msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [True, False]}\n",
        "                  }\n",
        "\n",
        "knn_param_grid = {'k': [15, 20, 25, 30, 40, 50, 60]}\n",
        "\n",
        "baseline_param_grid = {}\n",
        "slopeOne_param_grid = {}\n",
        "clustering_param_grid = {}\n",
        "\n",
        "algos = {\n",
        "    'svd' : (SVD, {'n_epochs': [20, 25]}),\n",
        "    # # 'svdpp' : (SVDpp, svd_param_grid),\n",
        "    'baseline' : (BaselineOnly, baseline_param_grid),\n",
        "    'slopeOne' : (SlopeOne, slopeOne_param_grid),\n",
        "    'coClustering' : (CoClustering, clustering_param_grid),\n",
        "    'kNNBasic' : (KNNBasic, knn_param_grid),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8hyQ6h32Pms"
      },
      "outputs": [],
      "source": [
        "def collaborative_filtering(trainset, testset, cf_model = \"svd\"):\n",
        "  print(\"\\n\" + \"-\" *5 + cf_model+\" algorithm using surprise package \" + \"-\" *5)\n",
        "  algo = GridSearchCV(algos[cf_model][0], algos[cf_model][1], measures=['rmse', 'mae'], cv=5, n_jobs=-1)\n",
        "  algo.fit(trainset)\n",
        "  best_algo = algo.best_estimator['rmse']\n",
        "  print(best_algo)\n",
        "  predictions = algo.fit(trainset.construct_testset(trainset.raw_ratings))\n",
        "  # predictions = algo.test(testset)\n",
        "  # rmse = accuracy.rmse(predictions)\n",
        "  # mae = accuracy.mae(predictions)\n",
        "  return algo.best_estimator['rmse'], algo.best_estimator['mae'], predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myG-o7lWAztb"
      },
      "outputs": [],
      "source": [
        "def hybrid_approach_train(trainset, testset, weights = []):\n",
        "  rmse_arr = []\n",
        "  mae_arr = []\n",
        "  for algo in algos:\n",
        "    start_time = time.time()\n",
        "    rmse, mae, predictions = collaborative_filtering(trainset, testset, cf_model = algo)\n",
        "    file = open(result_dir + algo + \"train_predictions\", 'wb')\n",
        "    pickle.dump(predictions, file)\n",
        "    file.close()\n",
        "    rmse_arr.append(rmse)\n",
        "    mae_arr.append(mae)\n",
        "    print(\"Elapsed Time: \", time.time() - start_time)\n",
        "\n",
        "  file = open(result_dir+'mae_arr', 'wb')\n",
        "  pickle.dump(mae_arr, file)\n",
        "  file.close()\n",
        "\n",
        "  file = open(result_dir+'rmse_arr', 'wb')\n",
        "  pickle.dump(rmse_arr, file)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT6p_z2NqzAS",
        "outputId": "162c451a-1f43-4d5b-ed2a-4026b618e521"
      },
      "outputs": [],
      "source": [
        "if not path.exists('ml-latest-small'):\n",
        "    print(\"Downloading Files for first time use: \")\n",
        "    download_file = requests.get('http://files.grouplens.org/datasets/movielens/ml-latest-small.zip')\n",
        "    zipped_file = zipfile.ZipFile(io.BytesIO(download_file.content)) # having First.csv zipped file.\n",
        "    zipped_file.extractall()\n",
        "\n",
        "#We do not want the data to be sampled everytime, else the predictions won't match with each other.\n",
        "df_train_test, df_val = get_train_test_data(new_sample = False)\n",
        "trainset, testset = convert_traintest_dataframe_forsurprise(df_train_test, df_val)\n",
        "# hybrid_approach_train(trainset, testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "GDctPUpzakjT",
        "outputId": "987a52ca-1265-490b-eb79-c5ec5135e928"
      },
      "outputs": [],
      "source": [
        "# Parameter space\n",
        "svd_param_grid = {'n_epochs': [20, 25], \n",
        "                  'lr_all': [0.007, 0.009, 0.01],\n",
        "                  'reg_all': [0.4, 0.6]}\n",
        "\n",
        "# svdpp_gs = GridSearchCV(SVDpp, svd_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\n",
        "# svdpp_gs.fit(trainset)\n",
        "\n",
        "svd_gs = GridSearchCV(SVD, svd_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5, refit=True)\n",
        "svd_gs.fit(trainset)\n",
        "file = open(\"/common/home/jrp328/Downloads/MDM/results/svdpp_grid_search\", \"wb\")\n",
        "pickle.dump(svd_gs, file)\n",
        "file.close()\n",
        "file = open(\"/common/home/jrp328/Downloads/MDM/results/svdpp_grid_search\",\"rb\")\n",
        "svdpp_gs = pickle.load(file)\n",
        "file.close()\n",
        "print('SVDpp - RMSE:', round(svdpp_gs.best_score['rmse'], 4), '; MAE:', round(svdpp_gs.best_score['mae'], 4))\n",
        "\n",
        "print('SVD   - RMSE:', round(svd_gs.best_score['rmse'], 4), '; MAE:', round(svd_gs.best_score['mae'], 4))\n",
        "\n",
        "print(\"------SVDpp-----\")\n",
        "print('RMSE =', svdpp_gs.best_params['rmse'])\n",
        "print('MAE =', svdpp_gs.best_params['mae'])\n",
        "\n",
        "print(\"------SVD-----\")\n",
        "print('RMSE =', svd_gs.best_params['rmse'])\n",
        "print('MAE =', svd_gs.best_params['mae'])\n",
        "\n",
        "param_grid = {'k': [15, 20, 25, 30, 40, 50, 60]}\n",
        "\n",
        "knnbasic_gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\n",
        "knnbasic_gs.fit(trainset)\n",
        "\n",
        "knnmeans_gs = GridSearchCV(KNNWithMeans, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\n",
        "knnmeans_gs.fit(trainset)\n",
        "\n",
        "knnz_gs = GridSearchCV(KNNWithZScore, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\n",
        "knnz_gs.fit(trainset)\n",
        "\n",
        "x = [15, 20, 25, 30, 40, 50, 60]\n",
        "y1 = knnbasic_gs.cv_results['mean_test_rmse']\n",
        "y2 = knnbasic_gs.cv_results['mean_test_mae']\n",
        "\n",
        "y3 = knnmeans_gs.cv_results['mean_test_rmse']\n",
        "y4 = knnmeans_gs.cv_results['mean_test_mae']\n",
        "\n",
        "y5 = knnz_gs.cv_results['mean_test_rmse']\n",
        "y6 = knnz_gs.cv_results['mean_test_mae']\n",
        "\n",
        "plt.figure(figsize=(18,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('K Neighbors vs RMSE', loc='center', fontsize=15)\n",
        "plt.plot(x, y1, label='KNNBasic', color='lightcoral', marker='o')\n",
        "plt.plot(x, y5, label='KNNWithZScore', color='indianred', marker='o')\n",
        "plt.plot(x, y3, label='KNNWithMeans', color='darkred', marker='o')\n",
        "plt.xlabel('K Neighbor', fontsize=15)\n",
        "plt.ylabel('RMSE Value', fontsize=15)\n",
        "plt.legend()\n",
        "plt.grid(ls='dotted')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('K Neighbors vs MAE', loc='center', fontsize=15)\n",
        "plt.plot(x, y2, label='KNNBasic', color='lightcoral', marker='o')\n",
        "plt.plot(x, y4, label='KNNWithMeans', color='indianred', marker='o')\n",
        "plt.plot(x, y6, label='KNNWithZScore', color='darkred', marker='o')\n",
        "plt.xlabel('K Neighbor', fontsize=15)\n",
        "plt.ylabel('MAE Value', fontsize=15)\n",
        "plt.legend()\n",
        "plt.grid(ls='dotted')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMImmRIPqG1d"
      },
      "outputs": [],
      "source": [
        "def compute_error(actual_ratings, estimate_ratings):\n",
        "\tratings = np.array(actual_ratings)\n",
        "\testimate = np.array(estimate_ratings)\n",
        "\n",
        "\trmse = np.sqrt(np.sum(np.square(np.subtract(ratings, estimate)))/np.size(ratings))\n",
        "\tmae = np.sum(np.abs(np.subtract(ratings, estimate)))/np.size(ratings)\n",
        "\n",
        "\treturn rmse, mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIaDxN0fqeOp"
      },
      "outputs": [],
      "source": [
        "def precision_recall_calculation(predictions, threshold=3.5):\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_predict_true = defaultdict(list)\n",
        "    for user_id, movie_id, true_rating, predicted_rating, _ in predictions:\n",
        "        user_predict_true[user_id].append((predicted_rating, true_rating))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for user_id, user_ratings in user_predict_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        no_of_relevant_items = sum((true_rating >= threshold) for (predicted_rating, true_rating) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top 10\n",
        "        no_of_recommended_items = sum((predicted_rating >= threshold) for (predicted_rating, true_rating) in user_ratings[:10])\n",
        "\n",
        "        # Number of relevant and recommended items in top 10\n",
        "        no_of_relevant_and_recommended_items = sum(((true_rating >= threshold) and (predicted_rating >= threshold)) for (predicted_rating, true_rating) in user_ratings[:10])\n",
        "\n",
        "        # Precision: Proportion of recommended items that are relevant\n",
        "        precisions[user_id] = no_of_relevant_and_recommended_items / no_of_recommended_items if no_of_recommended_items != 0 else 1\n",
        "\n",
        "        # Recall: Proportion of relevant items that are recommended\n",
        "        recalls[user_id] = no_of_relevant_and_recommended_items / no_of_relevant_items if no_of_relevant_items != 0 else 1\n",
        "\n",
        "    # Averaging the values for all users\n",
        "    average_precision=sum(precision for precision in precisions.values()) / len(precisions)\n",
        "    average_recall=sum(recall for recall in recalls.values()) / len(recalls)\n",
        "    F_score=(2*average_precision*average_recall) / (average_precision + average_recall)\n",
        "    \n",
        "    return [average_precision, average_recall, F_score]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gthZ1cBBBs_"
      },
      "outputs": [],
      "source": [
        "def hybrid_approach_test(trainset, testset, weights = []):\n",
        "  predictions_all = []\n",
        "  for algo in algos:\n",
        "    file = open(result_dir+algo+\"predictions\",'rb')\n",
        "    predictions_all.append(pickle.load(file))\n",
        "    file.close()\n",
        "    \n",
        "  file = open('/content/drive/MyDrive/MDM/results/mae_arr', 'rb')\n",
        "  mae_arr = pickle.load(file)\n",
        "  file.close()\n",
        "\n",
        "  file = open('/content/drive/MyDrive/MDM/results/rmse_arr', 'rb')\n",
        "  rmse_arr = pickle.load(file)\n",
        "  file.close()\n",
        "\n",
        "  actual_ratings = []\n",
        "  estimate_arr = []\n",
        "\n",
        "  for p in predictions_all[1]:\n",
        "    actual_ratings.append(p[2])\n",
        "\n",
        "  for i, predictions in enumerate(predictions_all):\n",
        "    estimate_arr.append([])\n",
        "    for p in predictions:\n",
        "      estimate_arr[i].append(p[3])\n",
        "\n",
        "  if len(weights) == 0:\n",
        "    total = 0\n",
        "    for i, (e,f) in enumerate(zip(rmse_arr, mae_arr)):\n",
        "      if i in [0, 1, 2, 3, 4, 5]:\n",
        "        total += (1)/((e) ** 1)\n",
        "\n",
        "    for i, (e,f) in enumerate(zip(rmse_arr, mae_arr)):\n",
        "      if i in [0, 1, 2, 3, 4, 5]:\n",
        "        weights.append((1)/(((e) ** 1) * total))\n",
        "      else:\n",
        "        weights.append(0)\n",
        "\n",
        "    hybrid_estimates = np.zeros(np.asarray(estimate_arr[0]).shape)\n",
        "\n",
        "    for i, estimate in enumerate(estimate_arr):\n",
        "      hybrid_estimates += np.multiply(estimate, weights[i])\n",
        "\n",
        "  print(weights)\n",
        "\n",
        "  hybrid_predictions = []\n",
        "\n",
        "  for p, h in zip(predictions_all[0], hybrid_estimates):\n",
        "    hybrid_predictions.append((p[0], p[1], p[2], h, p[4]))\n",
        "\n",
        "  rmse, mae = compute_error(actual_ratings, hybrid_estimates)\n",
        "  [precision, recall, F_score] = precision_recall_calculation(hybrid_predictions, threshold=3.5)\n",
        "\n",
        "  print(\"\\n\" + \"-\" *5 + \" Hybrid algorithm \" + \"-\" *5)\n",
        "  print(\"RMSE: \", rmse)\n",
        "  print(\"MAE: \", mae)\n",
        "  print(\"Precision: \", precision)\n",
        "  print(\"Recall: \", recall)\n",
        "  print(\"F-Score: \",F_score)\n",
        "\n",
        "  print(str(rmse) + \"\\t\" + str(mae) + \"\\t\" + str(precision) + \"\\t\" + str(recall) + \"\\t\" + str(F_score))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of MDM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
