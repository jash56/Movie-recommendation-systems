{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import sys\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.python_utils import binarize\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.models.ncf.ncf_singlenode import NCF\n",
        "from recommenders.models.sar import SAR\n",
        "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
        "from recommenders.utils.notebook_utils import is_jupyter\n",
        "from recommenders.datasets.python_splitters import python_chrono_split, python_stratified_split\n",
        "from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
        "                                                     recall_at_k, get_top_k_items,logloss)\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor as GBRT\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from surprise import Dataset, Reader, SVD\n",
        "\n",
        "SEED = 42\n",
        "MODEL_DIR = './models/'\n",
        "TRAIN_MODELS = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('ml-latest-small/ratings.csv', dtype = {\"userId\": int, \"movieId\": int, \"rating\": float, \"timestamp\": int,})\n",
        "df.columns = ['userID', 'itemID', 'rating', 'timestamp']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = df[['userID', 'rating']]\n",
        "t['rating'].plot.hist(bins = 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_errors(data, pred):\n",
        "    eval_rmse = rmse(data, pred, col_user='userID', col_item='itemID', col_rating='rating', col_prediction='prediction')\n",
        "    eval_mae = mae(data, pred, col_user='userID', col_item='itemID', col_rating='rating', col_prediction='prediction')\n",
        "    return eval_rmse, eval_mae\n",
        "\n",
        "def get_rank_metrics(data, pred, top_k=5):\n",
        "    eval_ndcg = ndcg_at_k(data, pred, col_user='userID', col_item='itemID', col_rating='rating', col_prediction='prediction', k=top_k)\n",
        "    eval_precision = precision_at_k(data, pred, col_user='userID', col_item='itemID', col_rating='rating', col_prediction='prediction', k=top_k)\n",
        "    eval_recall = recall_at_k(data, pred, col_user='userID', col_item='itemID', col_rating='rating', col_prediction='prediction', k=top_k, relevancy_method='top_k')\n",
        "    eval_f1 = (2 * eval_precision * eval_recall) / (eval_precision + eval_recall)\n",
        "    return eval_ndcg, eval_precision, eval_recall, eval_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create train test data for SAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = python_stratified_split(df, ratio=0.8, col_user='userID', col_item='itemID', seed=SEED)\n",
        "\n",
        "test = test[test[\"userID\"].isin(train[\"userID\"].unique())]\n",
        "test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]\n",
        "\n",
        "train_file = \"./train.csv\"\n",
        "test_file = \"./test.csv\"\n",
        "train.to_csv(train_file, index=False)\n",
        "test.to_csv(test_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SAR implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_MODELS:\n",
        "    sar_model = SAR(\n",
        "        col_user=\"userID\",\n",
        "        col_item=\"itemID\",\n",
        "        col_rating=\"rating\",\n",
        "        col_timestamp=\"timestamp\",\n",
        "        similarity_type=\"jaccard\", \n",
        "        time_decay_coefficient=30, \n",
        "        timedecay_formula=True,\n",
        "        normalize=True\n",
        "    )\n",
        "    with Timer() as train_time:\n",
        "        sar_model.fit(train)\n",
        "    print(\"Took {} seconds for training.\".format(train_time.interval))\n",
        "\n",
        "    file = open(MODEL_DIR+'sar', 'wb')\n",
        "    pkl.dump(sar_model, file)\n",
        "    file.close()\n",
        "    \n",
        "sar_model = pkl.load(open(MODEL_DIR+'sar', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SAR error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with Timer() as test_time:\n",
        "#     preds = sar_model.predict(train)\n",
        "# print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
        "\n",
        "# top_k = 10\n",
        "# eval_rmse, eval_mae = get_errors(train, preds)\n",
        "# top_k_rec = sar_model.recommend_k_items(train, top_k=top_k, remove_seen = False)\n",
        "# eval_ndcg, eval_precision, eval_recall = get_rank_metrics(train, top_k_rec, top_k)\n",
        "# print(\"Model:\\t\",\n",
        "#       \"\\nTop K:\\t%d\" % top_k,\n",
        "#       \"\\nNDCG:\\t%f\" % eval_ndcg,\n",
        "#       \"\\nPrecision@K:\\t%f\" % eval_precision,\n",
        "#       \"\\nRecall@K:\\t%f\" % eval_recall,\n",
        "#       \"\\nRMSE:\\t%f\" % eval_rmse,\n",
        "#       \"\\nMAE:\\t%f\" % eval_mae,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with Timer() as test_time:\n",
        "    preds = sar_model.predict(test)\n",
        "print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
        "\n",
        "top_k = 10\n",
        "eval_rmse, eval_mae = get_errors(test, preds)\n",
        "top_k_rec = sar_model.recommend_k_items(test, top_k=top_k, remove_seen = True)\n",
        "eval_ndcg, eval_precision, eval_recall, eval_f1 = get_rank_metrics(test, top_k_rec, top_k)\n",
        "print(\"Model:\\t\",\n",
        "      \"\\nTop K:\\t%d\" % top_k,\n",
        "      \"\\nNDCG:\\t%f\" % eval_ndcg,\n",
        "      \"\\nPrecision@K:\\t%f\" % eval_precision,\n",
        "      \"\\nRecall@K:\\t%f\" % eval_recall,\n",
        "      \"\\nRMSE:\\t%f\" % eval_rmse,\n",
        "      \"\\nMAE:\\t%f\" % eval_mae,\n",
        "      \"\\nF1-score:\\t%f\" % eval_f1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd_model = SVD(n_epochs=25, lr_all=0.01, reg_all=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reader = Reader(rating_scale=(0,5))\n",
        "\n",
        "trainset = Dataset.load_from_df(train[['userID', 'itemID', 'rating']], reader)\n",
        "train_list = trainset.construct_testset(trainset.raw_ratings)\n",
        "trainset = trainset.construct_trainset(trainset.raw_ratings)\n",
        "\n",
        "testset = Dataset.load_from_df(test[['userID', 'itemID', 'rating']], reader)\n",
        "testset = testset.construct_testset(testset.raw_ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd_model.fit(trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd_train_preds = svd_model.test(train_list)\n",
        "svd_train_pred = pd.DataFrame(svd_train_preds, columns=['userID', 'itemID', 'rating', 'svd_prediction', 'details'])\n",
        "svd_train_pred = svd_train_pred.drop(columns=['details', 'rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd_test_preds = svd_model.test(testset)\n",
        "svd_test_pred = pd.DataFrame(svd_test_preds, columns=['userID', 'itemID', 'rating', 'prediction', 'details'])\n",
        "top_k = 10\n",
        "eval_rmse, eval_mae = get_errors(svd_test_pred[['userID', 'itemID', 'rating']], svd_test_pred[['userID', 'itemID', 'prediction']])\n",
        "svd_test_top_k = svd_test_pred.sort_values(by=['prediction'], ascending=False).groupby('userID').head(5)\n",
        "eval_ndcg, eval_precision, eval_recall, eval_f1 = get_rank_metrics(svd_test_pred[['userID', 'itemID', 'rating']], svd_test_top_k[['userID', 'itemID', 'prediction']], top_k)\n",
        "print(\"Model:\\t\",\n",
        "      \"\\nTop K:\\t%d\" % top_k,\n",
        "      \"\\nNDCG:\\t%f\" % eval_ndcg,\n",
        "      \"\\nPrecision@K:\\t%f\" % eval_precision,\n",
        "      \"\\nRecall@K:\\t%f\" % eval_recall,\n",
        "      \"\\nRMSE:\\t%f\" % eval_rmse,\n",
        "      \"\\nMAE:\\t%f\" % eval_mae,\n",
        "      \"\\nF1-score:\\t%f\" % eval_f1,)\n",
        "svd_test_pred = svd_test_pred.rename({'prediction': 'svd_prediction'}, axis='columns')\n",
        "svd_test_pred = svd_test_pred.drop(columns=['details', 'rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Popularity feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "popularity = pd.DataFrame(df['itemID'].value_counts())\n",
        "popularity = popularity.reset_index()\n",
        "popularity.columns = ['itemID', 'popularity']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GBRT data prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sar_train = sar_model.predict(train)\n",
        "sar_train = sar_train.rename({'prediction': 'sar_prediction'}, axis='columns')\n",
        "\n",
        "gbrt_train = train.merge(popularity, on='itemID', how='left').merge(sar_train, on=['itemID', 'userID'], how='left').merge(svd_train_pred, on=['itemID', 'userID'], how='left')\n",
        "gbrt_train = gbrt_train.drop(columns=['timestamp'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sar_test = sar_model.predict(test)\n",
        "sar_test = sar_test.rename({'prediction': 'sar_prediction'}, axis='columns')\n",
        "\n",
        "gbrt_test = test.merge(popularity, on='itemID', how='left').merge(sar_test, on=['itemID', 'userID'], how='left').merge(svd_test_pred, on=['itemID', 'userID'], how='left')\n",
        "gbrt_test = gbrt_test.drop(columns=['timestamp'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hypertuning GBRT (hybrid model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_MODELS:\n",
        "    n_estimators = [100]\n",
        "    learn_rates = [0.05]\n",
        "    max_depths = [5]\n",
        "\n",
        "    param_grid = {'n_estimators': n_estimators,\n",
        "                'learning_rate': learn_rates,\n",
        "                'max_depth': max_depths,}\n",
        "\n",
        "    grid_search = GridSearchCV(GBRT(loss='huber'),param_grid, cv=5, return_train_score=True)\n",
        "    grid_search.fit(gbrt_train.drop(columns=['userID', 'itemID', 'rating'], axis=1), gbrt_train['rating'])\n",
        "\n",
        "    file = open(MODEL_DIR+'gbrt', 'wb')\n",
        "    pkl.dump(grid_search, file)\n",
        "    file.close()\n",
        "    \n",
        "grid_search = pkl.load(open(MODEL_DIR+'gbrt', 'rb'))\n",
        "gbrt = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# pred = gbrt.predict(gbrt_train.drop(columns=['userID', 'itemID', 'rating'], axis=1))\n",
        "# gbrt_train['prediction'] = pred\n",
        "# top_k = 10\n",
        "# eval_rmse, eval_mae = get_errors(gbrt_train[['userID', 'itemID', 'rating']], gbrt_train[['userID', 'itemID', 'prediction']])\n",
        "# gbrt_train_top_k = gbrt_train.sort_values(by=['prediction'], ascending=False).groupby('userID').head(5)\n",
        "# eval_ndcg, eval_precision, eval_recall = get_rank_metrics(gbrt_train[['userID', 'itemID', 'rating']], gbrt_train_top_k[['userID', 'itemID', 'prediction']], top_k)\n",
        "# print(\"Model:\\t\",\n",
        "#       \"\\nTop K:\\t%d\" % top_k,\n",
        "#       \"\\nNDCG:\\t%f\" % eval_ndcg,\n",
        "#       \"\\nPrecision@K:\\t%f\" % eval_precision,\n",
        "#       \"\\nRecall@K:\\t%f\" % eval_recall,\n",
        "#       \"\\nRMSE:\\t%f\" % eval_rmse,\n",
        "#       \"\\nMAE:\\t%f\" % eval_mae,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = gbrt.predict(gbrt_test.drop(columns=['userID', 'itemID', 'rating'], axis=1))\n",
        "gbrt_test['prediction'] = pred\n",
        "top_k = 10\n",
        "eval_rmse, eval_mae = get_errors(gbrt_test[['userID', 'itemID', 'rating']], gbrt_test[['userID', 'itemID', 'prediction']])\n",
        "gbrt_test_top_k = gbrt_test.sort_values(by=['prediction'], ascending=False).groupby('userID').head(5)\n",
        "eval_ndcg, eval_precision, eval_recall, eval_f1 = get_rank_metrics(gbrt_test[['userID', 'itemID', 'rating']], gbrt_test_top_k[['userID', 'itemID', 'prediction']], top_k)\n",
        "print(\"Model:\\t\",\n",
        "      \"\\nTop K:\\t%d\" % top_k,\n",
        "      \"\\nNDCG:\\t%f\" % eval_ndcg,\n",
        "      \"\\nPrecision@K:\\t%f\" % eval_precision,\n",
        "      \"\\nRecall@K:\\t%f\" % eval_recall,\n",
        "      \"\\nRMSE:\\t%f\" % eval_rmse,\n",
        "      \"\\nMAE:\\t%f\" % eval_mae,\n",
        "      \"\\nF1-score:\\t%f\" % eval_f1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gbrt_test_top_k[gbrt_test_top_k['userID'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for imp in gbrt.feature_importances_:\n",
        "    print(imp)\n",
        "print(gbrt_train.drop(['userID', 'itemID', 'rating'], axis=1).columns)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MDM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
